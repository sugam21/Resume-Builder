{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory for top level folder\n",
    "model_dir_ = \"/home/sugam/Work/10-19 NLP/12 Projects/Resume Builder/Output/\"\n",
    "data_dir =  \"/home/sugam/Work/10-19 NLP/12 Projects/Resume Builder/data/Processed/\"\n",
    "\n",
    "model = model_dir_ + \"bert/\"\n",
    "context_data = data_dir + \"context_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1\n",
    "- Load the context data\n",
    "- Do text preprocessing like lowercasing -> punctuation removal -> accent words removal -> stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load the data\n",
    "context = pd.read_csv(context_data)\n",
    "context.rename(columns={\"0\":\"context\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerCasing(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the string and converts into lower casing\"\"\"\n",
    "\n",
    "    def fit(self,text,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,text):\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovePunctuation(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes the string and removes punctuation\"\"\"\n",
    "    \n",
    "    def fit(self,text,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,text):\n",
    "        exclude = '!\"#$%&\\'()*+./:;<=>?@[\\\\]^`{|}~'\n",
    "        text = text.translate(str.maketrans(\"\",\"\",exclude))\n",
    "        text = re.sub(\",\",\" \",text)\n",
    "        text = re.sub(r\"\\(\",\" \",text)\n",
    "        text = re.sub(r\"\\)\",\" \",text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveAccent(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes string and removes accent words\"\"\"\n",
    "    \n",
    "    def fit(self,text,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,text):\n",
    "        accent_letters = 'éàáñüãèìöäøõîûçôšâ'\n",
    "        text = text.translate(str.maketrans(\"\",\"\",accent_letters))\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopWords(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"Takes the string and remove the stopwords\"\"\"\n",
    "    \n",
    "    def fit(self,text,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,text):\n",
    "        new_text = []\n",
    "        for words in text.split():\n",
    "            if words not in stopwords.words(\"english\"):\n",
    "                new_text.append(words)\n",
    "            else:\n",
    "                new_text.append(\"\")\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tokenize(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, text, y = None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, text):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"lower\",LowerCasing()),\n",
    "    (\"remove punctuation\",RemovePunctuation()),\n",
    "    (\"remove accent\",RemoveAccent()),\n",
    "    (\"remove stopwords\",RemoveStopWords())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;lower&#x27;, LowerCasing()),\n",
       "                (&#x27;remove punctuation&#x27;, RemovePunctuation()),\n",
       "                (&#x27;remove accent&#x27;, RemoveAccent()),\n",
       "                (&#x27;remove stopwords&#x27;, RemoveStopWords())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;lower&#x27;, LowerCasing()),\n",
       "                (&#x27;remove punctuation&#x27;, RemovePunctuation()),\n",
       "                (&#x27;remove accent&#x27;, RemoveAccent()),\n",
       "                (&#x27;remove stopwords&#x27;, RemoveStopWords())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LowerCasing</label><div class=\"sk-toggleable__content\"><pre>LowerCasing()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemovePunctuation</label><div class=\"sk-toggleable__content\"><pre>RemovePunctuation()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveAccent</label><div class=\"sk-toggleable__content\"><pre>RemoveAccent()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveStopWords</label><div class=\"sk-toggleable__content\"><pre>RemoveStopWords()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('lower', LowerCasing()),\n",
       "                ('remove punctuation', RemovePunctuation()),\n",
       "                ('remove accent', RemoveAccent()),\n",
       "                ('remove stopwords', RemoveStopWords())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------> DO NOT RUN 2X <-----------------------------------------------------------------------------------\n",
    "\n",
    "# Update each rows with preprocessed data -> pass each row through the pipeline\n",
    "context[\"context\"] = context[\"context\"].map(lambda x: pipe.fit_transform(x))\n",
    "\n",
    "\n",
    "\n",
    "#### Saving the processed context data and deleting the context variable to free up some memory\n",
    "context.to_csv(data_dir+\"context_processed.csv\",index=False)\n",
    "del context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2\n",
    "- Load the preprocessed context data\n",
    "- load the gensim word2vec 200 dim vectorizer model and vectorize each row\n",
    "- vectorize the question \n",
    "- Perform cosine similarity of the question vector with the context vector\n",
    "- Find the maximum similariy and take that as the context of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved data\n",
    "context = pd.read_csv(data_dir+\"context_processed.csv\")\n",
    "context_series = context[\"context\"].copy()\n",
    "del context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------> DO NOT RUN 2X <-----------------------------------------\n",
    "#### Import the gensim model\n",
    "wv = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save the model for future use\n",
    "wv.save(model_dir_+\"glove_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(model_dir_+\"glove_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "  \"\"\"Creates a vector from sentence \"\"\"\n",
    "  vector_size = wv.vector_size\n",
    "  wv_res = np.zeros(vector_size)\n",
    "  ctr = 1\n",
    "  for w in sent:\n",
    "    if w in wv:\n",
    "      ctr+=1\n",
    "      wv_res += wv[w]\n",
    "  wv_res = wv_res/ctr\n",
    "  return wv_res\n",
    "\n",
    "vec = context_series.map(sent_vec)\n",
    "vec.to_pickle(model_dir_+\"glove_encoding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### This will load premade embeddings\n",
    "with open(data_dir+\"glove_encoding.pkl\", \"rb\") as file:\n",
    "    vec = pickle.load(file)\n",
    "\n",
    "### Convert into dataframe\n",
    "vec = pd.DataFrame(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is job provided by Inbiopro Solutions Pvt Ltd company with autocad as input?\"\n",
    "question = pipe.fit_transform(question)\n",
    "question_vector = sent_vec(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec[\"cosine_similarity\"] = vec[\"context\"].apply(lambda x: 1 - cosine(x,question_vector))\n",
    "index_max_similarity = vec[\"cosine_similarity\"].argmax()\n",
    "index_max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read the original context data\n",
    "og_context = pd.read_csv(data_dir+\"context_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the context based on the maximum similarity\n",
    "context_sentence = og_context.iloc[index_max_similarity,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict =  [\n",
    "    {\n",
    "        \"context\": context_sentence,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"id\": \"0\",\n",
    "            }\n",
    "        ],\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
